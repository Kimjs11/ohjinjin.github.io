---
title: "Machine Learning(17)"
categories: 
  - MachineLearning
last_modified_at: 2020-06-20T23:21:00+09:00
toc: true
published: false
---

Intro
---
학교 수강과목에서 학습한 내용을 복습하는 용도의 포스트입니다.<br/>

기존에 수강했던 인공지능과목을 통해서나 혼자 공부했던 내용이 있지만 거기에 머신러닝 수업을 들어서 보충하고 싶어서 수강하게 되었습니다.<br/>

gitlab과 putty를 이용하여 교내 서버 호스트에 접속하여 실습하는 내용도 함께 기록하려고 합니다.<br/>

* [원격 실습환경구축 따라하기](https://ohjinjin.github.io/git/gitlab/)<br/>

* [Machine Learning(1) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-1/)<br/>

* [Machine Learning(2) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-2/)<br/>

* [Machine Learning(3) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-3/)<br/>

* [Machine Learning(4) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-4/)<br/>

* [Machine Learning(5) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-5/)<br/>

* [Machine Learning(6) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-6/)<br/>

* [Machine Learning(7) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-7/)<br/>

* [Machine Learning(8) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-8/)<br/>

* [Machine Learning(9) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-9/)<br/>

* [Machine Learning(10) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-10/)<br/>

* [Machine Learning(11) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-11/)<br/>

* [Machine Learning(12) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-12/)<br/>

* [Machine Learning(13) 포스트 보러가기](https://ohjinjin.github.io/machinelearning/machineLearning-13/)<br/>

위 링크들 중 ANN 모델에 대한 설명은 Machine Learning(9)에 있습니다.<br/>
<br/>


EM, GMM

우리가 EM 알고리즘으로 풀려는 것들은 대부분 global optima를 찾기 어려울 때 입니다.

Convex 형태가 아닐 경우가 그에 해당하겠지요.

물론 convex 형태에도 EM 알고리즘을쓸 수는 있어요

그럴 필요가 없을 뿐이에염

수학 개념 중 Jensen’s inequality라는 것이 있어요.



미적분학에서 배웠던 것 같으네요




1은 x와 y 중간값이라고 합시다. 2는f(x)와 f(y)의 중간값이라고 할 수 있어요.

2는 1보다 항상 같거나크다라는 말을 하는 거에요.

 

왜냐하면 함수 f가 convex 형태이기때문입니다.

E는 expectation을의미합니다.

위로 볼록한 concave 형태의 함수는 부등호 방향이 반대로 되겠지요?

대표적인 그러한 함수가 바로 log function 입니다.




자 왜 EM 알고리즘을 이야기하다가 위 수식을 이야기하게되었을까요?

우리가 구하고자하는 파라메터를 세타라고 둡시다.

그리고 파라메터 세타로부터 생성된 최적의 값을 z라고 해요.

이러한 z를 데이터로부터 계산해나갑니다. 그러면  z를 얻게되면 세타를 궁극적으로 얻을 수 있게된다는 것이죠.

토픽모델링 LDA에서도 마찬가지에요.

 

Likelihood 가 뭐에여 얼마나 데이터가 그럼직한가 였잖아염?

데이터 X에 대한 likelihood를모노함수 log 를 씌워 표현해줍니다.






토픽 1번에 의해서 생성될수도 잇고 2번에 의해서 생성될수도 있고 모든 단어들에 의해 생성될 수 있는것이거든 한 문장이 생성될 때?

전체 경우의 수가 분모가 되잖아염?

그게 p(X,Z|세타) 에서조건 부분을 말하고 있는 거에요.

그리고 분포 함수를 또다시 말해요.

토픽모델 LDA에서도 나왔던 내용이에요.

Z가 단어마다 샘플링되었었잖아요?

여러 개중에 스탑 해서 고른게 z라고 말햇던거기억나죠?

그럼 그 단어별로 막대길이 다르게해서 그렸던부분 그건 어디서 나온걸까여?

그게 사전지식이라고했었는데 그러한 분포가 있어야할 거아닌가욤

 

그게 바로 여기서는 q에 해당합니다.

여기서 젠슨스 인이퀄러티가 나오져.

부등호 기준 우변을 L이라고 둡시다.

좌변은 log likelihood져

 



kL divergence 죠!

저 오른쪽 그림에서

Loglikelihood가 높이가 높을수록 좋은거라고합시다

왜냐면 더 그럼직할수록 좋은걸 테니까 ㅇㅇ

그리고 L함수가 젠슨스에 의해서 log함수가concave니까 더 작을거잖아여 그럼 그 차이가 KL이되었다는말을 하는거에염

 

L과 KL에는 모두 q가 존재하고 L이 클수록 KL은작아지고 L이 작아질수록 KL은 커집니다.

이 두마리토끼를 동시에 절대 잡을 수 없고 우리는 KL을 작게 만들어서L을 키우고 싶은겁니다.

 

잠시 원활한 이해를 위해 이부분을 건너뛰고 E-step과 M-step의 요약을 먼저 봅시다.

 

저 빨간 꾸불꾸불한선이 가장우리가 얻고싶은 likelihood라고합시다.

현재 우리는 파란선이야.







세타 올드는 접점을 말하고 세타 뉴는 파란 컨케이브함수의 최대값이고

근데 가장 높은 부분에해당하는 초록색(사실 더 갈수잇음)까지 서서히 움직이면서 세타뉴를 갱신해가겠지요.

 

돌아와서.. KL함수를 그래서 0으로만들어주는애를 찾을거에요. 거리가 가장 작아지도록이요

그러려면 분모분자가 같아지면돼 그럼 1이되잖아 그럼 로그함수에 의해서0이될거니까!

그래서 p==q라고 써놓은 거에요.

E-step

 파라메터세타를 고정시킵니다. 세타 old로요

우리는 세타 old를 기반으로 q를구했습니다.

 

잘 이해하기 위해서 LDA에서의 예시를 봅시다

 

세타는 토픽 분포야 막대기로 생겼어

스탑해서 a는 카테고리 2번! 하고 골랐어

그래서 워드 분포 2에다가 a를썼어





확률 분포자체를 계산해낼 수도 있긴한데 어쩄든 보통 샘플링해서 단어 빈도 계산해서 과정자체가 Estep이에여 위예제에서는 방금 말한 과정 전체가 Estep입니다.

그러케 단어 네개에 대해서 2,1,3,1이렇게 매겼다고하면은 그게estep 끝난겨





Mstep은 분포보고 막대기 다시그리는 과정ㅇ이에여

 

EM algorithm에서는 이를

Mstep에서는 변수 Z고정하고MLE로써 세타 new를 계산한다 라고 표현합니다.

처음에는 파라메터를 가정하고 estep을 거쳐서 파라메터를 갱신하는mstep

왼발(파라메터)을 고정하고오른발(z)를 내밀고 이번엔 z를 고정하고 왼발을 내밀져

그걸 한번의 iteration이라고 합니다.

계속 반복해요. 특정 임계치가 될때까지

 






왼쪽이 분자부분, 오른쪽이 분모부분인데

오른쪽이 자세히보면 엔트로피거든여

q(z)=p(z)라고 가정했기 때문에 estep을 통해서 이미 정해져있어

그래서 걍 상수값입니다.

왼쪽 텀 같은 경우에는 우리가 구하고자하는 세타 new에 해당하는항이 있습니다.

 

그래서 왼쪽항을 다시 q라는 함수로 표현한거고 오른쪽은 상수constant라고 표현한거에염





GMM은 모델이고 EM은학습할 수 있는 알고리즘이에여

GMM(Gaussian Mixture Model)은 클러스터링 등에서사용되곤하는 모델중 하나입니다.

이를 이해하기 위한 상황은 아래와 같이 가정합니다.

Bi-modal dataset

한 개의 Gaussian 으로는 모델링이 불가능한 경우요.

가우시안 디스트리뷰션과 가우시안 모델 두개로부터 가정해야하는 상황이라고 해여

 

우리는 데이터가 가진 특징을 보고 골라줘야하는거에여 클러스터링은 특히 더 그래요.

데이터 특징을 잘 관찰하고 탐색적으로 데이터를 분석해보고 클러스터를 n개로정하고 그런거지염

이 경우에서는 클러스터 개수가 최적이 2개인 경우를 가정하자는 거에여

 

C라는 변수를 정할거고요 이게 앞서 EM알고리즘에서 말했던 Z에 해당하는 것입니다.

Bernoulli random variable로 두 모델 중 어느Gaussian인지를 나타내게 됩니다.

 

LDA로 비유하자면 어떤 토픽을 가리키는지가 Z였잖아여? 여기서는 어느 가우시안인지 가리키는 용도인겁니다.







빨간색 가우시안을 나타내는 확률을 파이라고 하면 바이 모달이니까 파란색 가우시안의 확률이 1-파이가 되겠져

 

1번가우시안에 의해서 데이터x가생성되었을 확률이 P(X|C=1)겠지염

데이터 x가 가우시안 2번에속할 확률이 얼마나되니 ? 이게 P(X|C=2)입니다.

가우시안 1에 의해서 x가생성된 x값이 얼마나 likely 하니…라는 뜻..

C가 1일 때 이 가우시안에의해서 x가 얼마나 그럼직하니

이 높이값이 그 확률 값이에여

그래서 가우시안 랜덤변수 역할을하는 것은 데이터 X입니다.






그럼 모델 파라미터를 다 정리해보면 파이, 뮤1, 시그마1, 뮤2, 시그마2 다섯개져

Prior는 가우시안1과2 중에 어느쪽이 더 대세니를 말하는 거에요.

그 정보가 P(c=1)에 들어갈거에요.

데이터 x가 있는데 그게 1번가우시안에서 얼마나 likely하니가

P(C=1)P(X=x|세타, C=1)입니다.

1번 가우시안에 해당하는게 저거고 오른쪽항은 2번 가우시안에 해당하는거고 두개를 더한다는 것이 전체 확률이 되겠져

왼쪽에 P(C=1)이 파이가 되었구 P(X=x|세타,C=1)이 N()으로바뀐거고 ㅎㅎ

LDA의 estimation 방법이 EM이라고 말할 수 있는건가염 ㅇㅇ 속한다고 볼수 있져

LDA 학습시키기위한 알고리즘은 여러가지가 있구 그중 딱하나 EM 소개시켜준거에여

그래서 결부시켜서 설명하는거에염

전체 데이터를 D라고 치환했어여

그럼 전체 데이터에 대한 라이클리후드에 log를 붙이면 더하기가 되져

앞서 우리가 정의햇던 식 그대로 들어간거고.. 똑같아여






cj는 j번째 데이터가가진 C값인거에여

c값이 1인 데이터의 개수를의미하는 것이 n1, c값이 2인 데이터들의 개수가 n2

 

동그라미하나하나가 데이터에여






C를 우리가 알고있다는 가정하에 우리가 이렇게 구할 수 있는거에요.

전체 데이터 n개중에서 1 그게파이겠져

 

위 식을 보면

c값을 알고나니까 모든 파라미터가 다 구해진다는 것을 알 수 잇어여

 

근데 문제는 c를 모른다는 거져

Soft clustering이라고해서 1번에 속할확률값, 2번에 속할 확률값을 표현하기도 하지만

어쨌든 C를 모른다구여

그래서 EM알고리즘을 씁니다.








종료 조건은 엔지니어가 주기 나름입니다.

우리가 배웠던거에 의하면 q==p라고 가정함으로써 C를 가지고 감마를 구하고 감마를 가지고 C를 구합니다.

i번째 데이터가 1번 가우시안일확률을 감마i로 표현하고있습니다.

감마라는 건 probability로 이야기하고 있습니다만 probability를 갖고있다하면 sampling을 할 수도 있는거에여

감마를 어쨌든 구해놓고 그 감마를 사용해서 mstep에서는 세타 new를구하게됩니다.

E라고 되어있는건 expectation을말하며 세타,D는 확률 분포를 말하는거에여

그거를 사용해서 C와 D에대한 joint probability를 구해서 expectation을최대화 시키는 세타를 구해라 라는 수식이 3번에 해당합니다.

두개의 가우시안이 mixture 되어있는 상황에서 군집화를 보여주는그림임다

11번 슬라이드 아래 그림

 

개념적으로 본거고

진짜 수식으로 함 봅시다.

EM은 어떻게 convex나 concave가 아닌 형태의 세타를 최적화할 수 있다는 건지 잘 이해가 안되었습니다.?????????

 

문제가

ANN에서 역전파랑 똑같아여

Optimization funcftion은

말이 모호하자나

최대화해야할 수도잇고 최소화해야핳ㄹ수동있거든

그걸 다합쳐서 최적화ㅏ라고하지

우리가 likelihood일때는 최대, loss/cost는 최소화시키고픈거고

어쨌든 최적화할 대상에 대한 함수를 그려보는데

시각화해서 보여주면 편하니까 2차원으로 마니 표현하져

J가 최소가 될수잇게하는 세타를 구하는건 이해가되지?

근데 만약 미분가능은한데 엄청 꾸불꾸불하게 아래처럼 생겻어






어케 최적값구할겨 못구해

근데 실제로 만나게되는 데이터들은 저런식으로 나오기도한다는겨

노이즈도 잇고 에러도잇고 ㅇㅇ

그러한 노이즈에대한영향ㅇ은 덜받으면서

최적화해야할 대상에 대한함수가 저렇게 생겻다고하면

최소화/최대화 시켜주는 세타를 찾을 수 잇느냐하는 문제에서

Global optima는 구할 수 없어

보통은 우리가 이런 함수를 구할 때 편미분=0을 두고 구하곤하자나

근데 미분이 너무 어려워서 못구해

이러한이유로 인해 글로벌옵티마도 못구하는겨

 

그래서 차선책이 머냐면

임의의 세타를 가정해놓고

거기서 접선기울기 따라서 그라디언트디센트저 그게 체인룰에 의해서 ㅇㅇ 백프로파게이션알고리즘이져

 

점점더 낮은곳으로가다가 그지점의 세터를 찾는거야

그렇게 로컬이더라도 옵티마를 구할 수 잇는거고

 

미분=0이렇게 구할수잇던 문제가 아니라는거야

 

오늘 배운것도 똑같아

엄청 꾸불꾸불해서 미분이 너무 어렵거나 =0으로 구할 수가 없는겨

그래서 Z랑 세타를 가지고 한스텝씩 가는겨

끝!

 

 

이 다음은 다음시간에..

 

 

ANN 실습마지막부분

Minmaxscaler를 통해 노말라이즈해주는거에여

Fit_transform했던 위치와 transform 했던 위치를 잘봐주세여

실습시에 이미 이야기했긴했지만

Fit_transform()은 학습도 하고 학습된걸기준으로 변환도해주는거거든여

0~1사이로 만들어주는거자나

상한선과 하한선을 만들어야 변환이 가능해지겠져

 

그 상한선 하한선을 구하는 과정을 스케일러입장에서는 학습이라고 부릅니다

스케일러 입장에서는 fit 함수에 의해 인자로 들어온 애를가지고 상한하한을 두는거고

 

Transform()으로 호출하는 경우는 앞서 만들어놓았던 fit 함수에의해 작업된 상한 하한선을 기준으로 변환만해주는거지

그게핵심이야

 

test에서는 fit안하고transform만해주는 것이ㅇㅇ

왜그럴까요 왜 핵심일까요?

 

바로 test이기 때문이져

Test 데이터는 unseen 데이터여야하자나여

학습데이터를 기반으로 성능을 평가해야하기 때문인거져

Test 데이터의 상한하한선까지 신경을 써서는 절대안됩니다.

 

 